{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4650bd36-7941-4693-a5b8-3bf1ca683ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Apple GPU via Metal\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8549c5b9-9ee0-49de-a499-c2e0bc1bccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Custom dataset class for text classification with tokenization.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: Any, tokenizer: Any, max_length: int = 150):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: Dataset containing 'label' and 'text' fields\n",
    "            tokenizer: Tokenizer for text processing\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "        \"\"\"\n",
    "        self.targets = torch.tensor(data['label'])\n",
    "        texts = data['text']\n",
    "        \n",
    "        tokens = tokenizer(\n",
    "            texts, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=max_length\n",
    "        )\n",
    "        \n",
    "        self.input_ids = tokens['input_ids']\n",
    "        self.attention_mask = tokens['attention_mask']\n",
    "        self.length = len(texts)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return self.input_ids[index], self.attention_mask[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf6c3b-83df-409d-8bc5-4b8a13515bc4",
   "metadata": {},
   "source": [
    "## Prepeare Torch Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8bab69-392b-4137-afc0-bb99d70292d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    \"\"\"Manages data loading and preprocessing for knowledge distillation.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str, tokenizer: Any, test_size: float = 0.2, \n",
    "                 max_length: int = 150, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Initialize data manager.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: Name of the dataset to load\n",
    "            tokenizer: Tokenizer for text processing\n",
    "            test_size: Fraction of data to use for validation\n",
    "            max_length: Maximum sequence length\n",
    "            batch_size: Batch size for data loaders\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_size = test_size\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_loader = None\n",
    "        self.valid_loader = None\n",
    "        self.test_loader = None\n",
    "    \n",
    "    def prepare_data(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Load and prepare data loaders.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train_loader, valid_loader, test_loader)\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        data = load_dataset(self.dataset_name)\n",
    "        \n",
    "        # Split data\n",
    "        train_test = data['train'].train_test_split(test_size=self.test_size, shuffle=True)\n",
    "        train_data = train_test['train']\n",
    "        valid_data = train_test['test']\n",
    "        test_data = data['test']\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        self._print_dataset_stats(train_data, valid_data, test_data)\n",
    "        \n",
    "        # Create custom datasets\n",
    "        train_dataset = TextDataset(train_data, self.tokenizer, self.max_length)\n",
    "        valid_dataset = TextDataset(valid_data, self.tokenizer, self.max_length)\n",
    "        test_dataset = TextDataset(test_data, self.tokenizer, self.max_length)\n",
    "        \n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size)\n",
    "        self.valid_loader = DataLoader(valid_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        return self.train_loader, self.valid_loader, self.test_loader\n",
    "    \n",
    "    def _print_dataset_stats(self, train_data: Any, valid_data: Any, test_data: Any):\n",
    "        \"\"\"Print dataset statistics.\"\"\"\n",
    "        print(f'Train set has {train_data.num_rows} samples')\n",
    "        print(f'Validation set has {valid_data.num_rows} samples')\n",
    "        print(f'Test set has {test_data.num_rows} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289c73b-d584-4312-b049-42236b1630d1",
   "metadata": {},
   "source": [
    "## Model Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ef8a67-87b3-4433-ba62-cddc25168689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Manages teacher and student models for knowledge distillation.\"\"\"\n",
    "    \n",
    "    def __init__(self, teacher_model_name: str, student_model_name: str, \n",
    "                 num_labels: int, device: torch.device, dropout_rate: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize model manager.\n",
    "        \n",
    "        Args:\n",
    "            teacher_model_name: Name/path of teacher model\n",
    "            student_model_name: Name/path of student model  \n",
    "            num_labels: Number of classification labels\n",
    "            device: Device to load models on\n",
    "            dropout_rate: Dropout rate for student model\n",
    "        \"\"\"\n",
    "        self.teacher_model_name = teacher_model_name\n",
    "        self.student_model_name = student_model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.device = device\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.teacher_model = None\n",
    "        self.student_model = None\n",
    "    \n",
    "    def load_models(self) -> Tuple[nn.Module, nn.Module]:\n",
    "        \"\"\"\n",
    "        Load teacher and student models.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (student_model, teacher_model)\n",
    "        \"\"\"\n",
    "        # Load student model\n",
    "        self.student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.student_model_name, \n",
    "            num_labels=self.num_labels\n",
    "        ).to(self.device)\n",
    "        self.student_model.dropout = nn.Dropout(self.dropout_rate)\n",
    "        \n",
    "        # Load teacher model\n",
    "        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.teacher_model_name,\n",
    "            num_labels=self.num_labels  \n",
    "        ).to(self.device)\n",
    "        \n",
    "        return self.student_model, self.teacher_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a8756-9943-421b-bf4a-b962f8bc9d45",
   "metadata": {},
   "source": [
    "## Metrics Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8610459a-f6b5-4215-b990-cc687e5c985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    \"\"\"Tracks and manages training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize metrics tracker.\"\"\"\n",
    "        self.training_loss_list = []\n",
    "        self.training_kd_loss_list = []\n",
    "        self.training_ce_loss_list = []\n",
    "        self.training_accuracy_list = []\n",
    "        self.valid_loss_list = []\n",
    "        self.valid_accuracy_list = []\n",
    "    \n",
    "    def update_metrics(self, train_loss: float, train_kd_loss: float, train_ce_loss: float,\n",
    "                      train_accuracy: float, valid_loss: float, valid_accuracy: float):\n",
    "        \"\"\"Update all metrics with current epoch values.\"\"\"\n",
    "        self.training_loss_list.append(train_loss)\n",
    "        self.training_kd_loss_list.append(train_kd_loss)\n",
    "        self.training_ce_loss_list.append(train_ce_loss)\n",
    "        self.training_accuracy_list.append(train_accuracy)\n",
    "        self.valid_loss_list.append(valid_loss)\n",
    "        self.valid_accuracy_list.append(valid_accuracy)\n",
    "    \n",
    "    def print_detailed_metrics(self, epoch: int, total_epochs: int, train_loss: float, \n",
    "                             train_kd_loss: float, train_ce_loss: float, train_accuracy: float,\n",
    "                             valid_loss: float, valid_accuracy: float):\n",
    "        \"\"\"Print detailed metrics for current epoch.\"\"\"\n",
    "        print(f\"\"\"\n",
    "        Epoch {epoch + 1}/{total_epochs}:\n",
    "        ├─ Combined Loss: {train_loss:.4f}\n",
    "        ├─ KD Loss: {train_kd_loss:.4f} \n",
    "        ├─ CE Loss: {train_ce_loss:.4f}\n",
    "        ├─ KD/CE Ratio: {train_kd_loss/train_ce_loss:.2f}\n",
    "        ├─ Train Accuracy: {train_accuracy:.4f}\n",
    "        ├─ Valid Loss: {valid_loss:.4f}\n",
    "        └─ Valid Accuracy: {valid_accuracy:.4f}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231cfc8-2ef6-4a30-b359-1e7f150fcd7c",
   "metadata": {},
   "source": [
    "## Knowldge Distillation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc3a61f-3d89-4fe4-9c6b-022e01a03971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationTrainer:\n",
    "    \"\"\"Main trainer class for knowledge distillation.\"\"\"\n",
    "    \n",
    "    def __init__(self, student_model: nn.Module, teacher_model: nn.Module, \n",
    "                 device: torch.device, temperature: float = 3.0, alpha: float = 0.7,\n",
    "                 learning_rate: float = 2e-5, epochs: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize knowledge distillation trainer.\n",
    "        \n",
    "        Args:\n",
    "            student_model: Student model to train\n",
    "            teacher_model: Teacher model for distillation\n",
    "            device: Device for training\n",
    "            temperature: Temperature for knowledge distillation\n",
    "            alpha: Weight for combining CE and KD losses\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            epochs: Number of training epochs\n",
    "        \"\"\"\n",
    "        self.student_model = student_model\n",
    "        self.teacher_model = teacher_model\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Loss functions\n",
    "        self.entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.kd_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "        # Metrics tracker\n",
    "        self.metrics_tracker = MetricsTracker()\n",
    "        \n",
    "        # Initialize optimizer and scheduler (will be set during training)\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "    \n",
    "    def accuracy_score(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], \n",
    "                      model: nn.Module) -> float:\n",
    "        \"\"\"Calculate accuracy for a batch.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[0].to(self.device), batch[1].to(self.device))\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            class_predictions = torch.argmax(probabilities, dim=1)\n",
    "            acc = torch.mean((class_predictions == batch[2].to(self.device)).to(torch.float)).data.item()\n",
    "            return acc\n",
    "    \n",
    "    def setup_optimizer_scheduler(self, train_loader: DataLoader):\n",
    "        \"\"\"Setup optimizer and learning rate scheduler.\"\"\"\n",
    "        self.optimizer = optim.AdamW(self.student_model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.learning_rate,\n",
    "            total_steps=len(train_loader) * self.epochs,\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float, float, float]:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.student_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_kd_loss = 0.0\n",
    "        train_ce_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            input_ids = batch[0].to(self.device)\n",
    "            attention_mask = batch[1].to(self.device)\n",
    "            target_tensors = batch[2].to(self.device)\n",
    "            \n",
    "            # Student model predictions\n",
    "            student_logits = self.student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            \n",
    "            # Cross-entropy loss\n",
    "            ce_loss = self.entropy_loss(student_logits, target_tensors)\n",
    "            \n",
    "            # Teacher model logits\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = self.teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "            \n",
    "            # Knowledge distillation loss\n",
    "            kd_loss = self.temperature ** 2 * self.kd_loss_fn(\n",
    "                F.log_softmax(student_logits / self.temperature, dim=-1),\n",
    "                F.softmax(teacher_logits / self.temperature, dim=-1)\n",
    "            )\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = self.alpha * ce_loss + (1. - self.alpha) * kd_loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Update training metrics\n",
    "            train_kd_loss += kd_loss.item()\n",
    "            train_ce_loss += ce_loss.item()\n",
    "            train_loss += loss.item()\n",
    "            accuracy = self.accuracy_score(batch, self.student_model)\n",
    "            train_accuracy += accuracy\n",
    "        \n",
    "        # Calculate averages\n",
    "        train_accuracy /= len(train_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "        train_kd_loss /= len(train_loader)\n",
    "        train_ce_loss /= len(train_loader)\n",
    "        \n",
    "        return train_loss, train_kd_loss, train_ce_loss, train_accuracy\n",
    "    \n",
    "    def validate_epoch(self, valid_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"Validate for one epoch.\"\"\"\n",
    "        self.student_model.eval()\n",
    "        valid_loss = 0.0\n",
    "        valid_accuracy = 0.0\n",
    "        \n",
    "        for batch in valid_loader:\n",
    "            input_ids = batch[0].to(self.device)\n",
    "            attention_mask = batch[1].to(self.device)\n",
    "            target_tensors = batch[2].to(self.device)\n",
    "            \n",
    "            output = self.student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            val_loss = self.entropy_loss(output.logits, target_tensors)\n",
    "            valid_loss += val_loss.item()\n",
    "            accuracy = self.accuracy_score(batch, self.student_model)\n",
    "            valid_accuracy += accuracy\n",
    "        \n",
    "        # Calculate averages\n",
    "        valid_accuracy /= len(valid_loader)\n",
    "        valid_loss /= len(valid_loader)\n",
    "        \n",
    "        return valid_loss, valid_accuracy\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, valid_loader: DataLoader) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: Training data loader\n",
    "            valid_loader: Validation data loader\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all training metrics\n",
    "        \"\"\"\n",
    "        # Setup optimizer and scheduler\n",
    "        self.setup_optimizer_scheduler(train_loader)\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs), total=self.epochs):\n",
    "            # Training\n",
    "            train_loss, train_kd_loss, train_ce_loss, train_accuracy = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            valid_loss, valid_accuracy = self.validate_epoch(valid_loader)\n",
    "            \n",
    "            # Print metrics\n",
    "            self.metrics_tracker.print_detailed_metrics(\n",
    "                epoch, self.epochs, train_loss, train_kd_loss, train_ce_loss,\n",
    "                train_accuracy, valid_loss, valid_accuracy\n",
    "            )\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics_tracker.update_metrics(\n",
    "                train_loss, train_kd_loss, train_ce_loss,\n",
    "                train_accuracy, valid_loss, valid_accuracy\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'training_loss': self.metrics_tracker.training_loss_list,\n",
    "            'training_kd_loss': self.metrics_tracker.training_kd_loss_list,\n",
    "            'training_ce_loss': self.metrics_tracker.training_ce_loss_list,\n",
    "            'training_accuracy': self.metrics_tracker.training_accuracy_list,\n",
    "            'valid_loss': self.metrics_tracker.valid_loss_list,\n",
    "            'valid_accuracy': self.metrics_tracker.valid_accuracy_list\n",
    "        }\n",
    "    \n",
    "    def save_model(self, save_path: str, tokenizer: Any):\n",
    "        \"\"\"Save the trained student model and tokenizer.\"\"\"\n",
    "        self.student_model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0eccc-c619-464b-8d55-723ba7c0ecbe",
   "metadata": {},
   "source": [
    "## Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c246fb-9372-4e7b-af2d-89b439a9f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationPipeline:\n",
    "    \"\"\"Complete pipeline for knowledge distillation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary containing all parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Run the complete knowledge distillation pipeline.\"\"\"\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config['student_model_name'])\n",
    "        \n",
    "        # Prepare data\n",
    "        data_manager = DataManager(\n",
    "            dataset_name=self.config['dataset_name'],\n",
    "            tokenizer=tokenizer,\n",
    "            test_size=self.config.get('test_size', 0.2),\n",
    "            max_length=self.config.get('max_length', 150),\n",
    "            batch_size=self.config.get('batch_size', 64)\n",
    "        )\n",
    "        train_loader, valid_loader, test_loader = data_manager.prepare_data()\n",
    "        \n",
    "        # Load models\n",
    "        model_manager = ModelManager(\n",
    "            teacher_model_name=self.config['teacher_model_name'],\n",
    "            student_model_name=self.config['student_model_name'],\n",
    "            num_labels=self.config['num_labels'],\n",
    "            device=self.device,\n",
    "            dropout_rate=self.config.get('dropout_rate', 0.2)\n",
    "        )\n",
    "        student_model, teacher_model = model_manager.load_models()\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = KnowledgeDistillationTrainer(\n",
    "            student_model=student_model,\n",
    "            teacher_model=teacher_model,\n",
    "            device=self.device,\n",
    "            temperature=self.config.get('temperature', 3.0),\n",
    "            alpha=self.config.get('alpha', 0.7),\n",
    "            learning_rate=self.config.get('learning_rate', 2e-5),\n",
    "            epochs=self.config.get('epochs', 3)\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        metrics = trainer.train(train_loader, valid_loader)\n",
    "        \n",
    "        # Save model\n",
    "        if 'save_path' in self.config:\n",
    "            trainer.save_model(self.config['save_path'], tokenizer)\n",
    "        \n",
    "        return metrics, trainer, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53061eb-1ab1-4099-b53a-971ab7a3d02b",
   "metadata": {},
   "source": [
    "## Run Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b28bca-7bdd-457b-b4cb-cece02425a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████▎                            | 1/3 [1:25:49<2:51:39, 5149.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 1/3:\n",
      "    ├─ Combined Loss: 0.5981\n",
      "    ├─ KD Loss: 1.1593 \n",
      "    ├─ CE Loss: 0.3576\n",
      "    ├─ KD/CE Ratio: 3.24\n",
      "    ├─ Train Accuracy: 0.8985\n",
      "    ├─ Valid Loss: 0.2353\n",
      "    └─ Valid Accuracy: 0.9369\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████▋              | 2/3 [2:29:30<1:12:47, 4367.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 2/3:\n",
      "    ├─ Combined Loss: 0.2024\n",
      "    ├─ KD Loss: 0.2559 \n",
      "    ├─ CE Loss: 0.1795\n",
      "    ├─ KD/CE Ratio: 1.43\n",
      "    ├─ Train Accuracy: 0.9537\n",
      "    ├─ Valid Loss: 0.1997\n",
      "    └─ Valid Accuracy: 0.9463\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [3:28:32<00:00, 4170.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Epoch 3/3:\n",
      "    ├─ Combined Loss: 0.1415\n",
      "    ├─ KD Loss: 0.1569 \n",
      "    ├─ CE Loss: 0.1349\n",
      "    ├─ KD/CE Ratio: 1.16\n",
      "    ├─ Train Accuracy: 0.9647\n",
      "    ├─ Valid Loss: 0.2049\n",
      "    └─ Valid Accuracy: 0.9480\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/arsalsyed/Documents/student_model_distilled/tokenizer_config.json',\n",
       " '/Users/arsalsyed/Documents/student_model_distilled/special_tokens_map.json',\n",
       " '/Users/arsalsyed/Documents/student_model_distilled/vocab.txt',\n",
       " '/Users/arsalsyed/Documents/student_model_distilled/added_tokens.json',\n",
       " '/Users/arsalsyed/Documents/student_model_distilled/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'dataset_name': 'ag_news',\n",
    "    'teacher_model_name': 'bert-base-uncased',  \n",
    "    'student_model_name': 'distilbert-base-uncased', \n",
    "    'num_labels': 4,  # Number of classes in ag_news\n",
    "    'test_size': 0.2, # train-validation split \n",
    "    'max_length': 150,\n",
    "    'batch_size': 64,  \n",
    "    'temperature': 3.0,\n",
    "    'alpha': 0.7,\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 3,  \n",
    "    'dropout_rate': 0.2, #set dropout for student model \n",
    "    'save_path': \"/Users/arsalsyed/Documents/student_model_distilled\"  # Path to save the trained model\n",
    "}\n",
    "\n",
    "# Initialize and run pipeline\n",
    "pipeline = KnowledgeDistillationPipeline(config)\n",
    "metrics, trainer, test_loader = pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071152f0-0b02-4585-8415-17a85d48a93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
