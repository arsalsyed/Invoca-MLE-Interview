{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723a476d-bf6f-4207-9aa0-ad719fc16f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8a561f-b92c-43f6-a460-914a2366a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Loading dataset...\n",
      "Starting Model Evaluation...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████| 7600/7600 [00:00<00:00, 25793.22 examples/s]\n",
      "/var/folders/lz/nn2nqnfs7wzd_x5y92n0q8jc0000gn/T/ipykernel_73473/773761140.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Performance ===\n",
      "Accuracy: 0.9447\n",
      "F1-Score: 0.9448\n",
      "Precision: 0.9449\n",
      "Recall: 0.9447\n",
      "\n",
      "=== Per-Class Performance ===\n",
      "      Class  Precision  Recall  F1-Score  Support\n",
      "0     World     0.9617  0.9516    0.9566     1900\n",
      "1    Sports     0.9874  0.9863    0.9868     1900\n",
      "2  Business     0.9212  0.9111    0.9161     1900\n",
      "3  Sci/Tech     0.9094  0.9300    0.9196     1900\n"
     ]
    }
   ],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model_path, dataset_name=\"ag_news\"):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with model path and dataset\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to your saved model directory\n",
    "            dataset_name (str): Dataset name (default: \"ag_news\")\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        print(\"Loading model and tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        \n",
    "        # Load dataset\n",
    "        print(\"Loading dataset...\")\n",
    "        self.dataset = load_dataset(dataset_name)\n",
    "        \n",
    "        # AG News class labels\n",
    "        self.class_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "        \n",
    "    def preprocess_data(self, examples):\n",
    "        \"\"\"Tokenize the input text\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"], \n",
    "            truncation=True, \n",
    "            padding=True,\n",
    "            max_length=128\n",
    "        )\n",
    "    \n",
    "    def compute_metrics(self, eval_pred):\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='weighted'\n",
    "        )\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Perform comprehensive model evaluation\"\"\"\n",
    "        \n",
    "        # Prepare test dataset\n",
    "        test_dataset = self.dataset[\"test\"].map(\n",
    "            self.preprocess_data, \n",
    "            batched=True\n",
    "        )\n",
    "        \n",
    "        # Set up trainer for evaluation\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./eval_output\",\n",
    "            per_device_eval_batch_size=32,\n",
    "            dataloader_drop_last=False,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "        )\n",
    "        \n",
    "        # Run evaluation\n",
    "        print(\"Running evaluation...\")\n",
    "        eval_results = trainer.evaluate(test_dataset)\n",
    "        \n",
    "        # Get predictions for detailed analysis\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "        y_true = predictions.label_ids\n",
    "        \n",
    "        return eval_results, y_pred, y_true\n",
    "    \n",
    "    def detailed_analysis(self, y_true, y_pred):\n",
    "        \"\"\"Perform detailed analysis with confusion matrix and per-class metrics\"\"\"\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=None\n",
    "        )\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results_df = pd.DataFrame({\n",
    "            'Class': self.class_names,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Support': support\n",
    "        })\n",
    "        \n",
    "        return cm, results_df\n",
    "\n",
    "    \n",
    "    def run_full_evaluation(self):\n",
    "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "        \n",
    "        print(\"Starting Model Evaluation...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Main evaluation\n",
    "        eval_results, y_pred, y_true = self.evaluate_model()\n",
    "        \n",
    "        # Print main metrics\n",
    "        print(f\"\\n=== Overall Performance ===\")\n",
    "        print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "        print(f\"F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "        print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "        print(f\"Recall: {eval_results['eval_recall']:.4f}\")\n",
    "        \n",
    "        # Detailed analysis\n",
    "        cm, results_df = self.detailed_analysis(y_true, y_pred)\n",
    "        \n",
    "        print(f\"\\n=== Per-Class Performance ===\")\n",
    "        print(results_df.round(4))\n",
    "        \n",
    "        \n",
    "        return eval_results, results_df\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual model path\n",
    "    MODEL_PATH = \"/Users/arsalsyed/distilbert-ag-news\"  # path to trained model\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator(MODEL_PATH)\n",
    "    \n",
    "    # Run full evaluation\n",
    "    results, per_class_results = evaluator.run_full_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359044ea-8ad5-403e-beb3-57baa0312856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca5cec-d539-456f-8684-f4bca6f95a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3ea02-1042-4c43-a643-7176cdabcc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692747e0-bb81-4a97-b970-0afcd810144d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716265c8-922d-4f83-bf95-51228492b916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de39bcc-8583-485a-98bc-da99a60e5eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b10a5-740a-49e8-9c8e-ddd784df3c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92529e1-55df-49e7-9bd2-41d71e7a1d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd83024-d322-45c1-90b1-a0aa3c80402c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c818c2-7b6d-4415-8548-c46f4496a11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168e637-77b9-4431-8b9f-558e94e966a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
